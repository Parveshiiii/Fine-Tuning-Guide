{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma_JWh-W-IF"
      },
      "source": [
        "**Hey eveyone this is Parveshii and you are on the nb where we will fine tune your favourite gpt oss 20b or 120b**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "GPT OSS is the open source model from OpenAI(ClosedAI) so as its a open source model we can fine tune it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "\n",
        "## **Getting started**\n",
        "</div>\n",
        "\n",
        "we will be using:\n",
        "1. Unsloth\n",
        "2. trl\n",
        "3. Transformers\n",
        "these are main lib that we gonna use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJr_9dXGpJ05",
        "outputId": "beb4aaca-6eac-4b30-f2ff-ea4308968122"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#lets download dependencies first\n",
        "!pip install -U unsloth trl tranformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "lets wrap the model in the unsloth fastlang wrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gE-Ez1qtyIA",
        "outputId": "e8f0c2b1-3c1e-4476-f8c2-d8f31cdb5e75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tok = FastLanguageModel(\n",
        "    model_id = \"openai/gpt-oss-20b\",   # or \"openai/gpt-oss-120b\"\n",
        "    max_seq_length = 2048,             # note: param name is max_seq_length\n",
        "    dtype = torch.float16,             # use torch.float16 for fp16\n",
        "    load_in_8bit = True,               # saves VRAM on T4\n",
        "    device_map = \"auto\",\n",
        "    full_finetuning = False            # keep False on T4, too small for full FT\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrWNr3MuFUS"
      },
      "source": [
        "lets test the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = tok.encode(\"hey bro\")\n",
        "m = tok.decode(t)\n",
        "print(m)"
      ],
      "metadata": {
        "id": "fI7ETe4SaErF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**setup of lora**"
      ],
      "metadata": {
        "id": "5Pnp5aRfcNLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "#this will only work is full_finetuning = False as it doesn't train the whole model"
      ],
      "metadata": {
        "id": "J9hUWOWkcP4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdRyKR44dcNI"
      },
      "source": [
        "#lets start to preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Load your dataset\n",
        "dataset = load_dataset(\"Parveshiiii/opencode_reasoning_filtered\")\n",
        "\n",
        "# Example formatting function\n",
        "def format_chat(example):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example[\"input\"]},#you cange this to the desired column you want to use as user or input whaih cvary in all dataset and its input in our case\n",
        "        {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
        "    ]\n",
        "    # Apply chat template → returns a single string in the model’s expected format\n",
        "    formatted = tok.apply_chat_template(messages, tokenize=False)\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "# Apply formatting\n",
        "dataset_train = dataset.map(format_chat)\n",
        "\n",
        "print(chat_ds[\"train\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "6Fvup2jFaVVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufxBm1yRnruN"
      },
      "source": [
        "#lets setup trl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset_train,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=4096,\n",
        "    packing=True,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_steps=10,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-5,\n",
        "        logging_steps=5,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs_smol_instruct_math\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "u4dCZJ4QbleC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
